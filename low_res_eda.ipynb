{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 11:28:58.363228: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-18 11:28:58.456206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 11:28:59.301352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import functools\n",
    "import shutil\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from climsim_utils.data_utils import *\n",
    "import fnmatch\n",
    "from etils import etree\n",
    "import time\n",
    "import gc\n",
    "import rich\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# from huggingface_hub import HfFileSystem\n",
    "# import time\n",
    "\n",
    "# # Initialize HfFileSystem\n",
    "# fs = HfFileSystem()\n",
    "\n",
    "# # Function to list files in a single directory\n",
    "# def list_directory(path):\n",
    "#     retry_count = 0\n",
    "#     max_retries = 5\n",
    "#     while retry_count < max_retries:\n",
    "#         try:\n",
    "#             items = fs.ls(path, detail=True)\n",
    "#             directories = [item['name'] for item in items if item['type'] == 'directory']\n",
    "#             print(directories)\n",
    "#             files = [item['name'] for item in items if item['type'] != 'directory']\n",
    "#             return directories, files\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error listing {path}: {e}\")\n",
    "#             retry_count += 1\n",
    "#             time.sleep(2 ** retry_count)  # Exponential backoff\n",
    "#     return [], []\n",
    "\n",
    "# # Function to list all files in a dataset repository using multithreading\n",
    "# def list_all_files_multithreaded(repo_id):\n",
    "#     stack = [f\"datasets/{repo_id}\"]\n",
    "#     all_files = []\n",
    "#     count = 0\n",
    "    \n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#         while stack:\n",
    "#             futures = {executor.submit(list_directory, path): path for path in stack}\n",
    "#             stack = []\n",
    "            \n",
    "#             for future in concurrent.futures.as_completed(futures):\n",
    "#                 directories, files = future.result()\n",
    "#                 stack.extend(directories)\n",
    "#                 all_files.extend(files)\n",
    "#                 count += len(files)\n",
    "                \n",
    "#                 if count % 100 == 0:\n",
    "#                     print(f\"Retrieved {count} files\")\n",
    "    \n",
    "#     return all_files\n",
    "\n",
    "# # List files in the dataset repository\n",
    "# repo_id = \"LEAP/ClimSim_low-res\"\n",
    "# file_paths = list_all_files_multithreaded(repo_id)\n",
    "\n",
    "# # Print the file paths\n",
    "# # for path in file_paths:\n",
    "# #     print(path)\n",
    "\n",
    "# # Save file paths to a pickle file\n",
    "# import pickle\n",
    "\n",
    "# with open('file_paths.pkl', 'wb') as f:\n",
    "#     pickle.dump(file_paths, f)\n",
    "\n",
    "# # Shorten file paths for easier directory structure inspection\n",
    "# shorten_fp = ['/'.join(f.split('/')[3:]) for f in file_paths]\n",
    "# shorten_fp = [f for f in shorten_fp if 'train' in f]\n",
    "# shorten_fp = sorted(shorten_fp)\n",
    "\n",
    "# # Print directory structure \n",
    "# def print_directory_structure(file_paths):\n",
    "#     directories = set()\n",
    "#     for f in file_paths:\n",
    "#         directory = '/'.join(f.split('/')[:-1])\n",
    "#         directories.add(directory)\n",
    "#     directories = sorted(directories)\n",
    "# print_directory_structure(shorten_fp)\n",
    "\n",
    "# np.save(\"all_files_in_low_res.npy\", shorten_fp, allow_pickle=True)\n",
    "# !mv all_files_in_low_res.npy climsim/all_files_in_low_res.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input files: 210240\n",
      "Total output files: 210240\n",
      "Total matched files: 183960\n",
      "Matching files took 0.14578962326049805 seconds\n",
      "Filtering files took 0.031223773956298828 seconds\n",
      "Total filtered files: 26280\n",
      "Files per split: 3285\n",
      "3285\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "import time\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "all_file_paths = np.load(\"climsim/all_files_in_low_res.npy\", allow_pickle=True)\n",
    "\n",
    "input_files = [f for f in all_file_paths if \"mli\" in f]\n",
    "output_files = set([f for f in all_file_paths if \"mlo\" in f])\n",
    "\n",
    "print(f\"Total input files: {len(input_files)}\")\n",
    "print(f\"Total output files: {len(output_files)}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "regexps = [\n",
    "        \"*/*/E3SM-MMF.mli.000[1234567]-*-*-*.nc\",  # years 1 through 7\n",
    "        \"*/*/E3SM-MMF.mli.0008-01-*-*.nc\",  # first month of year 8\n",
    "    ]\n",
    "matched_files = []\n",
    "for pattern in regexps:\n",
    "    matched_files.extend(fnmatch.filter(input_files, pattern))\n",
    "\n",
    "print(f\"Total matched files: {len(matched_files)}\")\n",
    "print(f\"Matching files took {time.time() - start} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "filtered_files = [\n",
    "    ip_f for ip_f in matched_files if ip_f.replace(\"mli\", \"mlo\") in output_files\n",
    "]\n",
    "print(f\"Filtering files took {time.time() - start} seconds\")\n",
    "\n",
    "filtered_files = filtered_files[::7]  # Time Stride\n",
    "filtered_files = np.array(filtered_files)\n",
    "\n",
    "print(f\"Total filtered files: {len(filtered_files)}\")\n",
    "\n",
    "n_splits = 8\n",
    "\n",
    "total_files = len(filtered_files)\n",
    "files_per_split = total_files // n_splits\n",
    "print(f\"Files per split: {files_per_split}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(filtered_files)\n",
    "\n",
    "splits = np.array_split(filtered_files, n_splits)\n",
    "splits = [s.tolist() for s in splits]\n",
    "\n",
    "print(len(splits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train/0006-06/E3SM-MMF.mli.0006-06-04-37200.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train/0006-06/E3SM-MMF.mli.0006-06-04-37200.nc',\n",
       "  'train/0002-10/E3SM-MMF.mli.0002-10-05-37200.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train/0002-10/E3SM-MMF.mli.0002-10-05-37200.nc',\n",
       "  'train/0003-01/E3SM-MMF.mli.0003-01-24-14400.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train/0003-01/E3SM-MMF.mli.0003-01-24-14400.nc',\n",
       "  'train/0006-06/E3SM-MMF.mlo.0006-06-04-37200.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train/0006-06/E3SM-MMF.mlo.0006-06-04-37200.nc',\n",
       "  'train/0002-10/E3SM-MMF.mlo.0002-10-05-37200.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train/0002-10/E3SM-MMF.mlo.0002-10-05-37200.nc',\n",
       "  'train/0003-01/E3SM-MMF.mlo.0003-01-24-14400.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train/0003-01/E3SM-MMF.mlo.0003-01-24-14400.nc',\n",
       "  'ClimSim_low-res_grid-info.nc': '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/ClimSim_low-res_grid-info.nc'},\n",
       " '/home/joylunkad/.cache/huggingface/hub/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from build_high_res_ds_local import get_dataset_from_file_names\n",
    "from typing import Any\n",
    "\n",
    "def get_dataset_from_file_names(\n",
    "    train_files,\n",
    "    repo_id=\"LEAP/ClimSim_high-res\",\n",
    ") -> dict[str, dict[str, Any]]:\n",
    "\n",
    "    download_file_fn = functools.partial(\n",
    "        hf_hub_download,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    def download_file(filename: str) -> str:\n",
    "        try:\n",
    "            return download_file_fn(filename=filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename} with error {e}\")\n",
    "            return None\n",
    "\n",
    "    download_paths = etree.parallel_map(download_file, train_files)\n",
    "    download_paths = [x for x in download_paths if x is not None]\n",
    "    file_paths = dict(zip(train_files, download_paths))\n",
    "    data_path = \"/\".join(file_paths[train_files[0]].split(\"/\")[:-3])\n",
    "    return file_paths, data_path\n",
    "\n",
    "\n",
    "train_files = splits[0][:3]\n",
    "\n",
    "train_files.extend([f.replace(\"mli\", \"mlo\") for f in train_files])\n",
    "train_files.append(\"ClimSim_low-res_grid-info.nc\")\n",
    "\n",
    "file_paths, data_path = get_dataset_from_file_names(\n",
    "    train_files, repo_id=\"LEAP/ClimSim_low-res\"\n",
    ")\n",
    "\n",
    "file_paths, data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 12:07:10.442964: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-18 12:07:13.183600: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-18 12:07:14.553647: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "norm_path = \"climsim/preprocessing/normalizations/\"\n",
    "input_mean = xr.open_dataset(norm_path + \"inputs/input_mean.nc\")\n",
    "input_max = xr.open_dataset(norm_path + \"inputs/input_max.nc\")\n",
    "input_min = xr.open_dataset(norm_path + \"inputs/input_min.nc\")\n",
    "output_scale = xr.open_dataset(norm_path + \"outputs/output_scale.nc\")\n",
    "\n",
    "grid_path = os.path.join(data_path, \"ClimSim_low-res_grid-info.nc\")\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "train_col_names = np.load(\"train_col_names.npy\").tolist()\n",
    "\n",
    "data = data_utils(\n",
    "    grid_info=grid_info,\n",
    "    input_mean=input_mean,\n",
    "    input_max=input_max,\n",
    "    input_min=input_min,\n",
    "    output_scale=output_scale,\n",
    ")\n",
    "\n",
    "data.set_to_v2_vars()\n",
    "\n",
    "data.normalize = False\n",
    "data.data_path = f\"{data_path}/*/\"\n",
    "\n",
    "data.set_regexps(\n",
    "    data_split=\"train\",\n",
    "    regexps=[\n",
    "        \"E3SM-MMF.mli.000[1234567]-*-*-*.nc\",  # years 1 through 7\n",
    "        \"E3SM-MMF.mli.0008-01-*-*.nc\",  # first month of year 8\n",
    "    ],\n",
    ")\n",
    "\n",
    "data.set_stride_sample(data_split=\"train\", stride_sample=1)\n",
    "data.set_filelist(data_split=\"train\")\n",
    "data_loader = data.load_ncdata_with_generator(data_split=\"train\")\n",
    "npy_iterator = list(data_loader.as_numpy_iterator())\n",
    "\n",
    "# filelist = np.array(\n",
    "#     [npy_iterator[x][2] for x in range(len(npy_iterator))]\n",
    "# ).flatten()\n",
    "# filelist = [x.decode() for x in filelist]\n",
    "# file_ids = [f.split(\"/\")[-1].split(\".\")[-2] for f in filelist]\n",
    "\n",
    "# train_index = [\n",
    "#     [\n",
    "#         f\"train_{file_ids[f_idx]}_{str(x)}\"\n",
    "#         for x in range(len(npy_iterator[f_idx][0]))\n",
    "#     ]\n",
    "#     for f_idx in range(len(file_ids))\n",
    "# ]\n",
    "# train_index = np.concatenate(train_index)\n",
    "\n",
    "# npy_input = np.concatenate([npy_iterator[x][0] for x in range(len(npy_iterator))])\n",
    "# print(\"dropping cam_in_SNOWHICE because of strange values\")\n",
    "# drop_idx = train_col_names.index(\"cam_in_SNOWHICE\")\n",
    "# npy_input = np.delete(npy_input, drop_idx, axis=1)\n",
    "# npy_output = np.concatenate([npy_iterator[x][1] for x in range(len(npy_iterator))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 557), (1152, 368), (1152,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_input = np.concatenate([npy_iterator[x][0] for x in range(len(npy_iterator))])\n",
    "npy_output = np.concatenate([npy_iterator[x][1] for x in range(len(npy_iterator))])\n",
    "\n",
    "grid_ids = np.arange(0, 384)\n",
    "grid_ids = np.broadcast_to(grid_ids, (len(npy_iterator), 384)).flatten()\n",
    "\n",
    "npy_input.shape, npy_output.shape, grid_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 557), (1152, 368))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_input = np.concatenate([npy_iterator[x][0] for x in range(len(npy_iterator))])\n",
    "npy_output = np.concatenate([npy_iterator[x][1] for x in range(len(npy_iterator))])\n",
    "\n",
    "npy_input.shape, npy_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test(a=1, b=2, c=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "@dataclass\n",
    "class Test:\n",
    "    a: int = 1\n",
    "    b: int = 2\n",
    "    c: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.c = self.a + self.b\n",
    "\n",
    "test = Test()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 6, 0, 7, 5, 4, 2, 3]),\n",
       " array([[ 0.75371362,  1.28111299, -1.37435467, ...,  1.53462682,\n",
       "          1.03634234, -1.33140981],\n",
       "        [ 2.01270494,  0.76469265, -2.29979263, ...,  0.27500362,\n",
       "          0.6303607 , -0.11230725],\n",
       "        [-0.23218852,  1.22245143,  0.04383246, ...,  0.54443714,\n",
       "         -1.00727361,  0.42453657],\n",
       "        ...,\n",
       "        [ 0.39611081, -0.22550056,  0.63230307, ..., -1.00806094,\n",
       "         -1.03804627,  0.48448222],\n",
       "        [-0.65697366, -0.38875687,  0.9545601 , ...,  1.34281384,\n",
       "         -1.09468071, -1.00750662],\n",
       "        [-0.01321841,  0.37577535, -0.62182652, ..., -1.03458696,\n",
       "          1.06802559, -0.30713247]]),\n",
       " array([[ 2.01270494e+00,  7.64692655e-01, -2.29979263e+00,\n",
       "          3.35268690e-01, -3.11094517e-01, -1.28600009e+00,\n",
       "          1.29145402e+00,  1.04409942e+00,  5.74793761e-01,\n",
       "          2.75003619e-01,  6.30360704e-01, -1.12307253e-01],\n",
       "        [ 3.72432391e-01, -3.91675625e-01, -2.85045704e+00,\n",
       "         -1.77669907e+00,  4.64291424e-02, -1.47531019e+00,\n",
       "         -4.13491449e-01, -9.83107690e-01, -1.70576042e+00,\n",
       "          3.76097563e-01, -9.14891657e-01, -1.04915593e+00],\n",
       "        [ 7.53713618e-01,  1.28111299e+00, -1.37435467e+00,\n",
       "         -8.39777982e-02, -2.64383016e-02,  5.07584920e-01,\n",
       "         -1.58171247e-01,  3.49958079e-01, -2.10943814e+00,\n",
       "          1.53462682e+00,  1.03634234e+00, -1.33140981e+00],\n",
       "        [ 4.57472985e-01, -9.72614058e-04,  5.71557276e-01,\n",
       "         -1.20808799e+00, -5.24745555e-01,  9.57479462e-01,\n",
       "         -2.49192227e-01, -8.66647373e-01, -7.54240841e-01,\n",
       "         -7.36811992e-01,  8.88764874e-01, -1.07990310e+00],\n",
       "        [ 5.33691329e-01,  1.27694246e+00, -1.19297719e+00,\n",
       "          1.01320146e+00, -9.36734572e-01, -7.72477763e-01,\n",
       "          4.68318524e-01,  4.24804345e-01, -1.02824988e+00,\n",
       "          3.44611272e-01,  1.18775742e+00,  9.89723234e-02],\n",
       "        [ 2.18063076e-02, -3.03131071e-01,  4.73333028e-01,\n",
       "         -4.45497286e-01,  2.25825633e-01,  5.84858226e-01,\n",
       "         -4.30964320e-01, -6.78181900e-01, -1.23657029e+00,\n",
       "          3.82630191e-01, -9.80962094e-01,  1.10073323e+00],\n",
       "        [-2.32188521e-01,  1.22245143e+00,  4.38324563e-02,\n",
       "          2.41311665e-01,  6.28788232e-02,  6.04356544e-01,\n",
       "         -8.30887631e-01,  7.46404281e-01,  1.44751407e+00,\n",
       "          5.44437145e-01, -1.00727361e+00,  4.24536568e-01],\n",
       "        [-8.41723341e-01, -1.33687822e-01, -7.16377638e-01,\n",
       "         -5.91597969e-01, -3.87756392e+00,  1.11324912e-01,\n",
       "         -1.08315286e+00,  5.93990569e-01, -6.44752124e-01,\n",
       "          1.30005800e-01, -2.23600168e-01, -1.32699900e+00]]),\n",
       " (8,),\n",
       " (384, 12))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "a = np.arange(8)\n",
    "np.random.shuffle(a)\n",
    "\n",
    "emb = np.random.normal(0, 1, size=(384, 12))\n",
    "\n",
    "a, emb, emb[a], a.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
